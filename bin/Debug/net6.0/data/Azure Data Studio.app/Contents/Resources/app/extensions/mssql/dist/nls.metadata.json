{"main":{"messages":["This sample code loads the file into a data frame and shows the first 10 results.","An error occurred converting the SQL document to a Notebook. Error : {0}","An error occurred converting the Notebook document to SQL. Error : {0}","Notebooks","Only .ipynb Notebooks are supported","Could not find the controller endpoint for this instance"],"keys":["msgSampleCodeDataFrame","mssql.errorConvertingToNotebook","mssql.errorConvertingToSQL","notebookFileType","unsupportedFileType","noController"]},"sqlToolsServer":{"messages":["{0} Started","Starting {0}","Failed to start {0}","Installing {0} to {1}","Installing {0}","Installed {0}","Downloading {0}","({0} KB)","Downloading {0}","Done installing {0}","Extracted {0} ({1}/{2})"],"keys":["serviceStartedStatusMsg","startingServiceStatusMsg","failedToStartServiceErrorMsg","installingServiceChannelMsg","installingServiceStatusMsg","installedServiceChannelMsg","downloadingServiceChannelMsg","downloadingServiceSizeChannelMsg","downloadingServiceStatusMsg","downloadServiceDoneChannelMsg","entryExtractedChannelMsg"]},"objectExplorerNodeProvider/hdfsCommands":{"messages":["All Files","Upload","Uploading files to HDFS","Upload operation was canceled","Error uploading files: {0}","Creating directory","Operation was canceled","Error on making directory: {0}","Enter directory name","Error on deleting files: {0}","Are you sure you want to delete this folder and its contents?","Are you sure you want to delete this file?","Saving HDFS Files","Save operation was canceled","Error on saving file: {0}","Generating preview","Error on previewing file: {0}","Error on copying path: {0}","An unexpected error occurred while opening the Manage Access dialog: {0}"],"keys":["allFiles","lblUploadFiles","uploading","uploadCanceled","uploadError","makingDir","mkdirCanceled","mkDirError","enterDirName","deleteError","msgDeleteFolder","msgDeleteFile","saving","saveCanceled","saveError","previewing","previewError","copyPathError","manageAccessError"]},"objectExplorerNodeProvider/objectExplorerNodeProvider":{"messages":["Please provide the username to connect to HDFS:","Please provide the password to connect to HDFS:","Session for node {0} does not exist","Error notifying of node change: {0}","HDFS","Root"],"keys":["promptUsername","prmptPwd","sessionNotFound","notifyError","hdfsFolder","rootLabel"]},"objectExplorerNodeProvider/command":{"messages":["$(sync~spin) {0}...","Cancel","Cancel operation?","Search Server Names"],"keys":["progress","cancelTooltip","cancel","mssql.searchServers"]},"sparkFeature/dialog/dialogCommands":{"messages":["Select other SQL Server","Please select SQL Server with Big Data Cluster.","No SQL Server is selected.","The selected server does not belong to a SQL Server Big Data Cluster","Error Get File Path: {0}"],"keys":["selectOtherServer","sparkJobSubmission.PleaseSelectSqlWithCluster","sparkJobSubmission.NoSqlSelected","errorNotSqlBigDataCluster","sparkJobSubmission.GetFilePathFromSelectedNodeFailed"]},"dashboard/serviceEndpoints":{"messages":["Metrics Dashboard","Log Search Dashboard","Spark Jobs Management and Monitoring Dashboard","Spark Diagnostics and Monitoring Dashboard","Copy","Application Proxy","Cluster Management Service","Gateway to access HDFS files, Spark","Management Proxy","Management Proxy","SQL Server Master Instance Front-End","Metrics Dashboard","Log Search Dashboard","Spark Diagnostics and Monitoring Dashboard","Spark Jobs Management and Monitoring Dashboard","HDFS File System Proxy","Proxy for running Spark statements, jobs, applications"],"keys":["grafana","kibana","sparkHistory","yarnHistory","copyText","endpoint.appproxy","endpoint.controller","endpoint.gateway","endpoint.managementproxy","endpoint.mgmtproxy","endpoint.sqlServerEndpoint","endpoint.grafana","endpoint.kibana","endpoint.yarnHistory","endpoint.sparkHistory","endpoint.webhdfs","endpoint.livy"]},"tableDesigner/tableDesigner":{"messages":["New Table"],"keys":["tableDesigner.NewTable"]},"telemetry":{"messages":["View Known Issues","{0} component exited unexpectedly. Please restart Azure Data Studio."],"keys":["viewKnownIssuesText","serviceCrashMessage"]},"features":{"messages":["Azure Data Studio needs to contact Azure Key Vault to access a column master key for Always Encrypted, but no linked Azure account is available. Please add a linked Azure account and retry the query.","Please select a linked Azure account:","Azure Data Studio needs to contact Azure Key Vault to access a column master key for Always Encrypted, but no linked Azure account was selected. Please retry the query and select a linked Azure account when prompted.","The configured Azure account for {0} does not have sufficient permissions for Azure Key Vault to access a column master key for Always Encrypted."],"keys":["mssql.missingLinkedAzureAccount","mssql.chooseLinkedAzureAccount","mssql.canceledLinkedAzureAccountSelection","mssql.insufficientlyPrivelagedAzureAccount"]},"localizedConstants":{"messages":["Node Command called without any node passed","Manage Access","Location : ","Permissions"," - Owner","Owner","Group"," - Owning Group","Everyone else","User","Group","Access","Default","Delete","Sticky Bit","Inherit Defaults","Read","Write","Execute","Add User or Group","Enter name","Add","Named Users and Groups","Default User and Groups","User or Group Icon","Apply","Apply Recursively","Unexpected error occurred while applying changes : {0}","Local file will be uploaded to HDFS. ",".......................... Submit Spark Job End ............................","Uploading file from local {0} to HDFS folder: {1}","Upload file to cluster Succeeded!","Upload file to cluster Failed. {0}","Submitting job {0} ... ","The Spark Job has been submitted.","Spark Job Submission Failed. {0} ","YarnUI Url: {0} ","Spark History Url: {0} ","Get Application Id Failed. {0}","Local file {0} does not existed. ","No SQL Server Big Data Cluster found.","Please connect to the Spark cluster before View {0} History.","Failed to find tenant '{0}' in account '{1}' when refreshing security token","{0} AAD token refresh failed, please reconnect to enable {0}","Editor token refresh failed, autocompletion will be disabled until the editor is disconnected and reconnected","Failed to find azure account {0} when executing token refresh"],"keys":["msgMissingNodeContext","mssql.manageAccessTitle","mssql.locationTitle","mssql.permissionsTitle","mssql.ownerPostfix","mssql.owner","mssql.group","mssql.owningGroupPostfix","mssql.everyone","mssql.userLabel","mssql.groupLabel","mssql.accessHeader","mssql.defaultHeader","mssql.delete","mssql.stickyHeader","mssql.inheritDefaultsLabel","mssql.readHeader","mssql.writeHeader","mssql.executeHeader","mssql.addUserOrGroup","mssql.enterNamePlaceholder","mssql.addLabel","mssql.namedUsersAndGroups","mssql.defaultUserAndGroups","mssql.userOrGroupIcon","mssql.apply","mssql.applyRecursively","mssql.errorApplyingAclChanges","sparkJobSubmission.LocalFileDestinationHint","sparkJobSubmission.SubmissionEndMessage","sparkJobSubmission.PrepareUploadingFile","sparkJobSubmission.UploadingFileSucceeded","sparkJobSubmission.UploadingFileFailed","sparkJobSubmission.PrepareSubmitJob","sparkJobSubmission.SubmitJobFinished","sparkJobSubmission.SubmitJobFailed","sparkJobSubmission.YarnUIMessage","sparkJobSubmission.SparkHistoryLinkMessage","sparkJobSubmission.GetApplicationIdFailed","sparkJobSubmission.LocalFileNotExisted","sparkJobSubmission.NoSqlBigDataClusterFound","sparkConnectionRequired","mssql.failedToFindTenants","mssql.tokenRefreshFailed","mssql.tokenRefreshFailedNoSecurityToken","mssql.failedToFindAccount"]},"objectExplorerNodeProvider/fileSources":{"messages":["NOTICE: This file has been truncated at {0} for preview. ","The file has been truncated at {0} for preview."],"keys":["maxSizeNotice","maxSizeReached"]},"sqlClusterLookUp":{"messages":["{0}Please provide the username to connect to the BDC Controller:","Please provide the password to connect to the BDC Controller","Error: {0}. ","Username and password are required"],"keys":["promptBDCUsername","promptBDCPassword","bdcConnectError","usernameAndPasswordRequired"]},"objectExplorerNodeProvider/connection":{"messages":["ConnectionInfo is undefined.","ConnectionInfo.options is undefined.","Some missing properties in connectionInfo.options: {0}"],"keys":["connectionInfoUndefined","connectionInfoOptionsUndefined","connectionInfoOptionsMissingProperties"]},"objectExplorerNodeProvider/hdfsProvider":{"messages":["Error: {0}","Cannot delete a connection. Only subfolders and files can be deleted."],"keys":["errorExpanding","errDeleteConnectionNode"]},"sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionDialog":{"messages":["Parameters for SparkJobSubmissionDialog is illegal","New Job","Cancel","Submit","{0} Spark Job Submission:",".......................... Submit Spark Job Start .........................."],"keys":["sparkJobSubmission.SparkJobSubmissionDialogInitializeError","sparkJobSubmission.DialogTitleNewJob","sparkJobSubmission.DialogCancelButton","sparkJobSubmission.DialogSubmitButton","sparkJobSubmission.SubmitSparkJob","sparkJobSubmission.SubmissionStartMessage"]},"hdfs/webhdfs":{"messages":["Invalid Data Structure","Unable to create WebHDFS client due to missing options: ${0}","'${0}' is undefined.","Bad Request","Unauthorized","Forbidden","Not Found","Internal Server Error","Unknown Error","Unexpected Redirect"],"keys":["webhdfs.invalidDataStructure","webhdfs.missingProperties","webhdfs.undefinedArgument","webhdfs.httpError400","webhdfs.httpError401","webhdfs.httpError403","webhdfs.httpError404","webhdfs.httpError500","webhdfs.unknownError","webhdfs.unexpectedRedirect"]},"objectExplorerNodeProvider/cancelableStream":{"messages":["Stream operation canceled by the user"],"keys":["streamCanceled"]},"prompts/confirm":{"messages":["Yes","No"],"keys":["msgYes","msgNo"]},"hdfs/hdfsModel":{"messages":["Applying permission changes recursively under '{0}'","Permission changes applied successfully.","Applying permission changes to '{0}'.","Error applying permission changes: {0}"],"keys":["mssql.recursivePermissionOpStarted","mssql.recursivePermissionOpSucceeded","mssql.recursivePermissionOpProgress","mssql.recursivePermissionOpError"]},"sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionModel":{"messages":["Parameters for SparkJobSubmissionModel is illegal","submissionArgs is invalid. ","submissionArgs is invalid. ","livyBatchId is invalid. ","Get Application Id time out. {0}[Log]   {1}","Property localFilePath or hdfsFolderPath is not specified. ","Property Path is not specified. "],"keys":["sparkJobSubmission.SparkJobSubmissionModelInitializeError","sparkJobSubmission.submissionArgsIsInvalid","sparkJobSubmission.submissionArgsIsInvalid","sparkJobSubmission.LivyBatchIdIsInvalid","sparkJobSubmission.GetApplicationIdTimeOut","sparkJobSubmission.localFileOrFolderNotSpecified.","sparkJobSubmission.PathNotSpecified."]},"sparkFeature/dialog/sparkJobSubmission/sparkConfigurationTab":{"messages":["GENERAL","Enter a name ...","Job Name","Spark Cluster","Path to a .jar or .py file","The selected local file will be uploaded to HDFS: {0}","JAR/py File","Main Class","Arguments","Command line arguments used in your main class, multiple arguments should be split by space.","Property Job Name is not specified.","Property JAR/py File is not specified.","Property JAR/py File is not specified.","Property Main Class is not specified.","{0} does not exist in Cluster or exception thrown. ","The specified HDFS file does not exist. ","Select","Error in locating the file due to Error: {0}"],"keys":["sparkJobSubmission.GeneralTabName","sparkJobSubmission.JobNamePlaceHolder","sparkJobSubmission.JobName","sparkJobSubmission.SparkCluster","sparkJobSubmission.FilePathPlaceHolder","sparkJobSubmission.LocalFileDestinationHintWithPath","sparkJobSubmission.MainFilePath","sparkJobSubmission.MainClass","sparkJobSubmission.Arguments","sparkJobSubmission.ArgumentsTooltip","sparkJobSubmission.NotSpecifyJobName","sparkJobSubmission.NotSpecifyJARPYPath","sparkJobSubmission.NotSpecifyJARPYPath","sparkJobSubmission.NotSpecifyMainClass","sparkJobSubmission.HDFSFileNotExistedWithPath","sparkJobSubmission.HDFSFileNotExisted","sparkSelectLocalFile","sparkJobSubmission.SelectFileError"]},"sparkFeature/dialog/sparkJobSubmission/sparkJobSubmissionService":{"messages":["No Spark job batch id is returned from response.{0}[Error] {1}","No log is returned within response.{0}[Error] {1}"],"keys":["sparkJobSubmission.LivyNoBatchIdReturned","sparkJobSubmission.LivyNoLogReturned"]},"sparkFeature/dialog/sparkJobSubmission/sparkAdvancedTab":{"messages":["ADVANCED","Reference Jars","Jars to be placed in executor working directory. The Jar path needs to be an HDFS Path. Multiple paths should be split by semicolon (;)","Reference py Files","Py Files to be placed in executor working directory. The file path needs to be an HDFS Path. Multiple paths should be split by semicolon(;)","Reference Files","Files to be placed in executor working directory. The file path needs to be an HDFS Path. Multiple paths should be split by semicolon(;)","Driver Memory","Amount of memory to allocate to the driver. Specify units as part of value. Example 512M or 2G.","Driver Cores","Amount of CPU cores to allocate to the driver.","Executor Memory","Amount of memory to allocate to the executor. Specify units as part of value. Example 512M or 2G.","Executor Cores","Amount of CPU cores to allocate to the executor.","Executor Count","Number of instances of the executor to run.","Queue Name","Name of the Spark queue to execute the session in.","Configuration Values","List of name value pairs containing Spark configuration values. Encoded as JSON dictionary. Example: '{\"name\":\"value\", \"name2\":\"value2\"}'."],"keys":["sparkJobSubmission.AdvancedTabName","sparkJobSubmission.ReferenceJarList","sparkJobSubmission.ReferenceJarListToolTip","sparkJobSubmission.ReferencePyList","sparkJobSubmission.ReferencePyListTooltip","sparkJobSubmission.ReferenceFilesList","sparkJobSubmission.ReferenceFilesListTooltip","sparkJobSubmission.driverMemory","sparkJobSubmission.driverMemoryTooltip","sparkJobSubmission.driverCores","sparkJobSubmission.driverCoresTooltip","sparkJobSubmission.executorMemory","sparkJobSubmission.executorMemoryTooltip","sparkJobSubmission.executorCores","sparkJobSubmission.executorCoresTooltip","sparkJobSubmission.executorCount","sparkJobSubmission.executorCountTooltip","sparkJobSubmission.queueName","sparkJobSubmission.queueNameTooltip","sparkJobSubmission.configValues","sparkJobSubmission.configValuesTooltip"]}}